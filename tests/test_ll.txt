   >>> from lurrn import sparsmat
   >>> from alphabet import CPPUniAlphabet
   >>> from lurrn.semkernel import PolynomialKernel, JSDKernel, MinKernel


We first create some vectors to test on - one that has all the mass concentrated on
the first dimension, one with all on the second, and two that are somewhere in-between...

   >>> mat2=sparsmat.CSRMatrixD()
   >>> mat2=mat2.fromVectors([sparsmat.SparseVectorD([(1,1)]),sparsmat.SparseVectorD([(2,1)]),sparsmat.SparseVectorD([(1,0.33),(2,0.66)]),sparsmat.SparseVectorD([(1,0.66),(2,0.33)]),sparsmat.SparseVectorD([])])
   >>> list(mat2)
   [SparseVectorD([(1,1.0)]), SparseVectorD([(2,1.0)]), SparseVectorD([(1,0.33),(2,0.66)]), SparseVectorD([(1,0.66),(2,0.33)]), SparseVectorD([])]

A polynomial kernel uses the dot product on unnormalized or L2-normalized data

   >>> k=PolynomialKernel(mat2)
   >>> mat2[0].dotSparse(mat2[1])
   0.0
   >>> mat2[1].dotSparse(mat2[1])
   1.0
   >>> k.kernel(0,0)
   1.0
   >>> k.kernel(1,1)
   1.0
   >>> k.kernel(0,1)
   0.0
   >>> k.kernel(0,4)
   0.0
   >>> print '%.2f'%(k.kernel(0,2))
   0.33
   >>> print '%.2f'%(k.kernel(1,2))
   0.66
   >>> print '%.2f'%(k.kernel(2,3))
   0.44

   >>> k=PolynomialKernel(mat2, normalize=True)
   >>> mat2[0].dotSparse(mat2[1])
   0.0
   >>> mat2[1].dotSparse(mat2[1])
   1.0
   >>> k.kernel(0,0)
   1.0
   >>> k.kernel(1,1)
   1.0
   >>> k.kernel(0,1)
   0.0
   >>> k.kernel(0,4)
   0.0
   >>> print '%.2f'%(k.kernel(0,2))
   0.45
   >>> print '%.2f'%(k.kernel(1,2))
   0.89
   >>> print '%.2f'%(k.kernel(2,3))
   0.80

jsd_unnorm and the JSDKernel use the Jensen-Shannon divergence
   >>> print mat2[0].jsd_unnorm(mat2[0])
   0.0
   >>> print mat2[1].jsd_unnorm(mat2[0])
   1.0
   >>> print mat2[0].jsd_unnorm(mat2[1])
   1.0
   >>> k=JSDKernel(mat2)
   >>> k.kernel(0,1)
   0.0
   >>> k.kernel(0,0)
   1.0
   >>> k.kernel(0,4)
   0.0
   >>> k.kernel(4,0)
   0.0
   >>> print '%.2f'%(k.kernel(0,2))
   0.54
   >>> print '%.2f'%(k.kernel(1,2))
   0.81
   >>> print '%.2f'%(k.kernel(2,3))
   0.92

MinKernel uses the minimum of two feature values (corresponds to L1-distance)
   >>> k=MinKernel(mat2)
   >>> k.kernel(0,1)
   0.0
   >>> k.kernel(0,0)
   1.0
   >>> k.kernel(0,4)
   0.0
   >>> print '%.2f'%(k.kernel(0,2))
   0.33
   >>> print '%.2f'%(k.kernel(1,2))
   0.66
   >>> print '%.2f'%(k.kernel(2,3))
   0.66

The min_vals method returns a SparseVector with the smaller value of each for common features
   >>> mat2[0].min_vals(mat2[2])
   SparseVectorD([(1,0.33)])
   >>> mat2[1].min_vals(mat2[2])
   SparseVectorD([(2,0.66)])
   >>> mat2[3].min_vals(mat2[2])
   SparseVectorD([(1,0.33),(2,0.33)])
